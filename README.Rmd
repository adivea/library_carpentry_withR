---
title: "README"
author: "john little"
date: "`r Sys.Date()`"
output: github_document
---

<!-- .md file is auto generate from the .Rmd file.  Do note edit the .md file.  Do edit and knit the .Rmd file -->

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
```

This is an exploration of how to bootstrap an *Introduction to R* workshop for [Library Carpentry](https://librarycarpentry.org/)

## Assumption

- Library data is not unique
- Library data may have some consistently unique challenges
- Data savvy skills should emphasize technique, tools, and approach

    - Techniques:  Reproducible, Scripting
    - Tools:  R / RStudio / Tidyverse
    - Approach:  Open Source and CC-BY are foundational.  
    
- For anyone intending to develop core competency as a data savvy staffer, i.e. become more than a comfortable beginner, using a fully capable data science scripting language is the best midterm investment from a skill building perspective. 
    
Narrative:  R has become and tool of choice for data science explorations.  **R** - a statistical programming language - and **Tidyverse** - a consistent way of applying and learning data science - are an excellent toolkit that can can be broadly applied when producing (and documenting) data summation projects.  This toolkit is an excellent option for advanced beginners to advanced intermediates.  For one, you can build on all that you learn, rather than have to learn 12 different applications.  Additionally, R as a  tool, along with version control, lends itself to reproducibility, portfolio building, on the way writing reports in various formats (MS Word, PPT, slides, web pages, blogs, dashboards, PDF, etc.).  As you become engaged with these best practices you will spend less time on the data science and more time engaging with the data.  You'll write various reports from one script  (web scrape, web publishing, PDF docs) by employing extensible data savvy techniques.  While R (or Python) can have a steep learning curve, the tool will carry a data miner farther than entry level tools -- which, while designed for easy of use, quickly reach an efficiency barrier).  What better to use scalable techniques that were built for sharing, documentation, reproducibility, openness, and collaboration.

## Process

1. [x] Explore OpenRefine CrossRef dataset  (exploring_crossref.Rmd)
1. [x] Fork Data Carpentry's _R for Soc Sci_
1. [x] Apply OpenRefine Examples over top of DC's _R for Soc Sci_
1. [ ] Pitch to A.Zoss
1. [x] CE will begin a loop in at his end)
1. [ ] Engage with people working on the [Library Carpentry R lesson](https://github.com/LibraryCarpentry/lc-r)
1. [ ] What can be done efficiently?
1. [ ] Pitch back to C.Erdmann 


## Example Output

```{r echo=FALSE, fig.width=8}
plot1 <- ggplot(data = fct_count(starwars$eye_color, sort = T) %>% head(6), aes(x = f, y = n)) +
  geom_col() +
  coord_flip() +
  xlab(NULL) +
  ylab("count")

plot2 <- ggplot(data = starwars, ) +
  geom_bar(aes(x = fct_rev(fct_lump(fct_infreq(eye_color), n = 5)))) +
  geom_bar(data = starwars %>% filter(eye_color == "orange"), aes(eye_color), fill = "orange") +
  annotate("text", label = "Orange eyes\n       are rare", x = 2, y = 4.2) +
  coord_flip() +
  labs(title = "Eye color of Star Wars Characters",
       x = "", y = "",
       caption = "data source: dplyr::starwars") +
  theme_bw()
  

plot3 <- ggplot(data = starwars, aes(x = fct_lump(fct_infreq(eye_color), n = 5))) +
  geom_bar(aes(fill = gender)) +
  xlab(NULL)

ggpubr::ggarrange(plot1, plot2)

plot3
```
